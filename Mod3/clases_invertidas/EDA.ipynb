{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T√©cnicas comunes utilizadas: FILOSOF√çA\n",
    "\n",
    "1. Resumen estad√≠stico para comprender la distribuci√≥n y variabilidad de los datos: medidas descriptivas como la media, mediana, desviaci√≥n est√°ndar, rango, percentiles\n",
    "\n",
    "2. Visualizaci√≥n de datos para visualizar las caracter√≠sticas de los datos y explorar las relaciones entre las variables: gr√°ficos y diagramas, gr√°ficos de dispersi√≥n, diagramas de caja, diagramas de barras, gr√°ficos de l√≠nea\n",
    "\n",
    "3. An√°lisis de correlaci√≥n entre las variables para identificar posibles relaciones y dependencias. Mediante el c√°lculo de coeficientes de correlaci√≥n, como el coeficiente de correlaci√≥n de Pearson.\n",
    "\n",
    "4. Tratamiento de valores at√≠picos o extremos que puedan tener un impacto significativo en los resultados del an√°lisis. Estos valores pueden ser errores de medici√≥n o indicativos de fen√≥menos inusuales o interesantes.\n",
    "\n",
    "5. Imputaci√≥n de datos faltantes: t√©cnicas para estimar o imputar los valores faltantes de manera que no afecten significativamente los resultados del an√°lisis.\n",
    "\n",
    "6. Segmentaci√≥n y agrupaci√≥n: t√©cnicas para identificar subgrupos o patrones particulares dentro del conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"PASOS\"\n",
    "- Entender las variables/columnas: en esta parte entenderemos qu√© variables tenemos, qu√© tipos de datos, si tenemos valores nulos o duplicados, qu√© valores √∫nicos tenemos en nuestro dataset, etc.\n",
    "\n",
    "- Limpiar el dataset: una vez identificados todos los valores √∫nicos, los duplicados y/o nulos procederemos a dejar nuestro dataset lo m√°s limpio posible, es decir, quitar duplicados, nulos, columnas irrelevantes etc.\n",
    "\n",
    "- Analizar las relaciones entre las variables: nos permitir√° encontrar patrones, relaciones o anomal√≠as en nuestros datos que nos ayudar√°n a sacar conclusiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øPara qu√© usaremos el EDA?\n",
    "\n",
    "1. Explorar Datos: Aprender√©is a realizar un primer an√°lisis a los datos, identificando patrones, valores at√≠picos y tendencias.\n",
    "\n",
    "2. Hacer Preguntas Significativas: os ayudar√° a formular preguntas significativas sobre los datos y dise√±ar estrategias para responderlas.\n",
    "\n",
    "3. Preparaci√≥n de Datos: Comprender√©is la calidad de los datos y c√≥mo abordar problemas como datos faltantes o valores at√≠picos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ventajas del EDA:\n",
    "El EDA es fundamental por varias razones:\n",
    "1. Toma de Decisiones Informadas: Nos permite tomar decisiones basadas en datos s√≥lidos y evidencias concreta.\n",
    "2. Identificaci√≥n de Oportunidades: Puede revelar oportunidades ocultas y patrones que no ser√≠an evidentes de otra manera.\n",
    "3. Comunicaci√≥n de Resultados: Prepara el terreno para presentar hallazgos de manera efectiva a otros stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el conjunto de datos, las variables \"job\" (tipo de trabajo), \"marital\" (estado civil), \"education\" (educaci√≥n)-, \"default\" (cr√©dito en incumplimiento), \"housing\" (pr√©stamo hipotecario), \"loan\" (pr√©stamo personal), \"contact\" (tipo de comunicaci√≥n de contacto), \"month\" (mes), \"day_of_week\" (d√≠a de la semana) y \"poutcome\" (resultado de la campa√±a anterior) son ejemplos de variables categ√≥ricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nominales: Son variables categ√≥ricas sin un orden inherente entre las categor√≠as. Las categor√≠as representan diferentes grupos o clases, pero no hay una relaci√≥n de jerarqu√≠a entre ellas. Ejemplos de variables nominales en el conjunto de datos proporcionado podr√≠an ser \"job\" (tipo de trabajo) y \"marital\" (estado civil)\n",
    "\n",
    "- Ordinales: Son variables categ√≥ricas con un orden o jerarqu√≠a entre las categor√≠as. Las categor√≠as representan diferentes niveles o rangos, y el orden entre ellas es significativo. Ejemplos de variables ordinales en el conjunto de datos podr√≠an ser \"education\" (educaci√≥n), donde las categor√≠as pueden indicar niveles educativos ascendentes (por ejemplo, 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'professional.course', 'university.degree'), o \"poutcome\" (resultado de la campa√±a anterior), donde las categor√≠as pueden ser 'failure', 'nonexistent' y 'success'\n",
    "\n",
    "- Binarias: Son un caso especial de variables categ√≥ricas que toman solo dos categor√≠as posibles. Las categor√≠as se pueden representar como '0' y '1', 'no' y 'yes', o cualquier otra combinaci√≥n de dos valores opuestos. Un ejemplo de variable binaria en el conjunto de datos es \"default\" (cr√©dito en incumplimiento), que puede tener los valores 'no', 'yes' o 'unknown'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Estad√≠stica Descriptiva: proceso de resumir y organizar datos para comprender sus caracter√≠sticas, son medidas de tendencia central (media, mediana, moda) y medidas de dispersi√≥n (varianza, desviaci√≥n est√°ndar).\n",
    "\n",
    "B. Estad√≠stica Inferencial: hacer inferencias o predicciones sobre una poblaci√≥n a partir de una muestra de datos: pruebas de hip√≥tesis, intervalos de confianza y regresi√≥n.\n",
    "\n",
    "C. An√°lisis Exploratorio de Datos (EDA): examinar y visualizar datos para identificar patrones, tendencias y posibles valores at√≠picos. Gr√°ficos, diagramas de dispersi√≥n y res√∫menes estad√≠sticos son herramientas comunes en EDA.\n",
    "\n",
    "D. Validaci√≥n y Limpieza de Datos: implica asegurarse de que los datos sean precisos y confiables, mientras que la limpieza implica eliminar valores at√≠picos, datos duplicados o inconsistencias en los datos\n",
    "\n",
    "E. √âtica en el An√°lisis de Datos: Comprender las implicaciones √©ticas de la recopilaci√≥n y el uso de datos es crucial. Esto incluye la privacidad de los datos y la responsabilidad en el manejo de la informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTENDER LOS DATOS\n",
    "Asegurar la consistencia y precisi√≥n de los datos. ¬øprecisos, completos y √∫tiles para el an√°lisis?\n",
    "Para asegurar la consistencia de los datos debemos validar:\n",
    "\n",
    "- Modelo de datos: Saber la intenci√≥n de los datos, es decir el ¬øpor que fueron colectados esos datos?¬øQue se busca responder con estos datos?¬øEstos datos hacen satisfacen los requerimientos necesarios para responder mis preguntas?.\n",
    "\n",
    "- Seguimiento de formato est√°ndar de archivos: verifica que la extensi√≥n de los archivos que estas manejando correspondan con el formato interno que tienen. Aseg√∫rate que los n√∫meros se expresen en el formato que estas trabajando.\n",
    "\n",
    "**Tipos de Datos:** tipos de dato booleano, entero, flotante, etc. Deben corresponder como fueron definidos. No debe de haber un flotante en una casilla para los booleanos.\n",
    "\n",
    "- Rango de variables: verifica que las variables est√©n dentro del rango establecido en la recolecci√≥n de datos. En caso de encontrar variables fuera del rango preg√∫ntate: como llegaron esos datos aqu√≠? tienen alg√∫n significado alterno? debo preservarlos o eliminarlos?\n",
    "\n",
    "- Unicidad: verifica que tan √∫nicos son los datos. detecta si existe duplicidad en los datos y corrige.\n",
    "\n",
    "- Consistencia de expresiones: Mantener el mismo \"formato\" al escribir fechas, horas, etc. Es decir, mantener en todos los registros el formato DD/MM/AA, o cualquier otro pero que sea consistente. De igual manera para las variables categ√≥ricas y booleanas (T o F, True o False, etc.).\n",
    "\n",
    "- Valores nulos: expl√≠citos o impl√≠citos en el dataset. Son datos faltantes. Porque esta vac√≠o? puedo rellenarlo con otro dato? esta vac√≠o por un proceso aleatorio o tiene un sentido?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entender las variables/columnas del dataset\n",
    "Tareas clave:\n",
    "\n",
    "1. Identificar las variables üîç: examinar el conjunto de datos para identificar todas las variables o columnas disponibles. Revisar la descripci√≥n del dataset o explorar los encabezados de las columnas para entender qu√© se est√° midiendo o registrando.\n",
    "\n",
    "2. Tipos de datos üóÇÔ∏è: conocer el tipo de datos que contiene cada columna. Algunas variables ser√°n num√©ricas, otras categ√≥ricas, algunas binarias, ordinales o incluso de tiempo. Entender esto nos ayudar√° a seleccionar las t√©cnicas adecuadas de an√°lisis y visualizaci√≥n m√°s adelante.\n",
    "\n",
    "3. Valores duplicados üîÑ: En esta fase, buscamos la presencia de valores duplicados en las columnas. Esto significa verificar si hay registros exactamente iguales o muy similares. Los valores duplicados pueden distorsionar nuestros an√°lisis, as√≠ que es crucial detectarlos para decidir si los eliminamos o investigamos su causa.\n",
    "\n",
    "4. Valores √∫nicos üéØ: Aqu√≠ analizamos los valores √∫nicos presentes en cada variable. Saber cu√°ntos y cu√°les son los valores √∫nicos nos puede ayudar a entender mejor la variabilidad de los datos. Tambi√©n es √∫til para detectar patrones o identificar posibles anomal√≠as.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "-cargamos el csv con el que vamos a ir desarrollando la lecci√≥n de hoy, recordad que la descripci√≥n la ten√©is al inicio del jupyter. \n",
    "-para eso usaremos el m√©todo \"pd.read_csv\" que ya vimos en la leccion anterior.\n",
    "\n",
    "df = pd.read_csv(\"files/bank-additional.csv\", index_col = 0)\n",
    "\n",
    "-hacemos un \".head()\" para mostrar las 5 primeras filas del dataframe\n",
    "df.head()\n",
    "-si nos fijamos bien, no podemos ver todas las columnas del DataFrame (hay '...' en mitad de nuestra tabla) \n",
    "-¬øqu√© podemos hacer para conseguir que se vean todas las gr√°ficas? \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "-despu√©s de aplicar el m√©todo ya podremos ver todas las columnas del df.\n",
    "\n",
    "df.head()\n",
    "\n",
    "-recordemos que tambi√©n tenemos el m√©todo \".tail()\", el cual nos mostrar√° las √∫ltimas 5 filas del dataframe\n",
    "\n",
    "df.tail()\n",
    "\n",
    "-a veces puede que no nos interese mostrar las 5 primeras o 5 √∫ltimas filas de un dataframe, y puede que solo queramos mostrar las dos primeras o las dos √∫ltimas\n",
    "-esto lo podemos hacer tambi√©n con \".head()\" o \".tail()\", incluyendo el n√∫mero de filas que queremos ver dentro del par√©ntesis\n",
    "\n",
    "df.head(2) # en este caso mostramos solo las dos primeras filas\n",
    "\n",
    "-o solo la √∫ltimas 3 filas\n",
    "\n",
    "df.tail(3)\n",
    "\n",
    "-¬øy si quisieramos ver \"x\" filas al azar de nuestro conjunto de datos? Podemos usar el m√©todo \".sample()\", el cual por defecto nos devuelve solo una fila\n",
    "\n",
    "df.sample()\n",
    "\n",
    "-si quisieramos ver 4 filas al azar de nuestro conjunto de datos solo tendr√≠amos que especificarlo dentro de los par√©ntesis\n",
    "\n",
    "df.sample(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Ahora presentamos otro metodo que nos servira para conocer mejor nuestro dataframe, el m√©todo\".shape\" \n",
    "-que nos permit√≠a saber cu√°ntas filas y columnas tenemos en el DataFrame. \n",
    "-üìå Recordemos que nos devuelve una tupla! donde su primer valor son las filas y el segundo valor las columnas.\n",
    "\n",
    "print(f\"El n√∫mero de filas que tenemos es {df.shape[0]}, y el n√∫mero de columnas es {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-otro de los m√©todos que m√°s usaremos es el m√©todo que nos devuelve el nombre de todas las columnas de nuestro DataFrame. Este es el m√©todo '.columns' \n",
    "-nos va a devolver un array con todos los nombres de las columnas\n",
    "\n",
    "df.columns\n",
    "\n",
    "\n",
    "- Conociendo la posicion de una columna en particular\n",
    "\n",
    "df.columns.get_loc(\"duration\") # el resultado sera 8 porque es su posicion.\n",
    "\n",
    "#utilizamos el metodo '.describe()' para sacar los principales estad√≠sticos del DataFrame\n",
    "#por defecto, nos devolver√° solo los estad√≠sticos de las VARIABLES NUM√âRICAS\n",
    "\n",
    "df.describe()\n",
    "\n",
    "#podemos usar el m√©todo '.T' para transponer los resultados\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¬øQu√© es lo que vemos aqu√≠?**\n",
    "\n",
    "Entendamos los resultados de algunas de las columnas, por ejemplo `age`: \n",
    "\n",
    "- `count`: El n√∫mero de observaciones no nulas en la columna \"age\".\n",
    "\n",
    "- `mean`: La media de las edades en la columna.\n",
    "\n",
    "- `std`: La desviaci√≥n est√°ndar, que mide la dispersi√≥n de las edades.\n",
    "\n",
    "- `min`: La edad m√≠nima en el conjunto de datos.\n",
    "\n",
    "- `25%`: El percentil 25, que representa el valor por debajo del cual se encuentra el 25% de las edades.\n",
    "\n",
    "- `50%`: La mediana o percentil 50, que es el valor que divide el conjunto de datos en dos mitades iguales.\n",
    "\n",
    "- `75%`: El percentil 75, que representa el valor por debajo del cual se encuentra el 75% de las edades.\n",
    "\n",
    "- `max`: La edad m√°xima en el conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hagamos lo mismo para las columnas categ√≥ricas, para eso tendremos que incluir entre los par√©ntesis el par√°metro include = \"object\"\n",
    "\n",
    "df.describe(include = \"object\").T\n",
    "\n",
    "\n",
    "**Entendamos los resultados para la columna `job`**\n",
    "\n",
    "\n",
    "- `count`: El n√∫mero de observaciones no nulas en la columna \"job\".\n",
    "\n",
    "- `unique`: La cantidad de valores √∫nicos en la columna \"job\".\n",
    "\n",
    "- `top`: El valor m√°s com√∫n en la columna \"job\" (en este caso, \"admin.\").\n",
    "\n",
    "- `freq`: La frecuencia del valor m√°s com√∫n (en este caso, \"admin.\").\n",
    "\n",
    "\n",
    "En el an√°lisis preliminar de datos, existen dos m√©todos muy interesantes: `unique()` y `.value_counts()`.\n",
    "\n",
    "El m√©todo `unique()` se utiliza para obtener los valores √∫nicos de una columna. Devuelve un *array* de los valores distintos presentes en esa columna, sin repetir. Ser√° √∫til para identificar las categor√≠as √∫nicas en una variable categ√≥rica o los diferentes elementos en una columna de texto. \n",
    "\n",
    "Por otro lado, el m√©todo `.value_counts()` nos permite contar la frecuencia de cada valor en una columna. Devuelve una serie que muestra el valor como √≠ndice y el recuento como valor. Es √∫til para obtener informaci√≥n sobre la distribuci√≥n de los valores en una columna, especialmente en variables categ√≥ricas. \n",
    "\n",
    "Ambos m√©todos son herramientas poderosas para explorar y comprender la naturaleza de los datos, y nos permiten obtener informaci√≥n clave sobre la distribuci√≥n y la unicidad de los valores en nuestras variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recordemos el DataFrame**\n",
    "\n",
    "df.head(1)\n",
    "\n",
    "#saquemos los valores √∫nicos para la columna de `marital` usando el m√©todo 'unique()'\n",
    "\n",
    "df[\"marital\"].unique()\n",
    "\n",
    "#y si usamos el '.value_counts()' \n",
    "#en este caso vemos que dentro de nuestro conjunto de datos tenemos 25999 casados, 12105 solteros y 4811 divorciados\n",
    "\n",
    "df[\"marital\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad nos interesar√≠a hacerlo para todas las variables categ√≥ricas del DataFrame, ya que en realidad queremos es explorar todas las columnas categ√≥ricas. Por lo tanto, lo que tendremos que hacer es: \n",
    "\n",
    "- Seleccionar las variables que son categ√≥ricas de nuestro DataFrame, para esto usaremos el m√©todo `.select_dtypes()` que es una funci√≥n en Pandas que nos permite seleccionar columnas de un DataFrame bas√°ndose en su tipo de datos. Devuelve un DataFrame que contiene solo las columnas que cumplen con los tipos de datos especificados en los argumentos `include` y `exclude`. La sintaxis b√°sica del m√©todo `select_dtypes()` es la siguiente:\n",
    "\n",
    "    ```python\n",
    "    DataFrame.select_dtypes(include=None, exclude=None)\n",
    "    ```\n",
    "\n",
    "    - `include` (opcional): Especifica los tipos de datos que se deben incluir en la selecci√≥n. Puede ser una cadena de texto o una lista de tipos de datos, como `'int'`, `'float'`, `'object'`, etc. Si no se proporciona este argumento, se seleccionar√°n todas las columnas que coincidan con los tipos de datos especificados en `exclude`.\n",
    "\n",
    "    - `exclude` (opcional): Especifica los tipos de datos que se deben excluir de la selecci√≥n. Puede ser una cadena de texto o una lista de tipos de datos. Si no se proporciona este argumento, se seleccionar√°n todas las columnas que coincidan con los tipos de datos especificados en `include`.\n",
    "\n",
    "\n",
    "    Algunos ejemplos de c√≥mo se puede utilizar el m√©todo `select_dtypes()`:\n",
    "\n",
    "    ```python\n",
    "    # Seleccionar todas las columnas num√©ricas\n",
    "    df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "    # Seleccionar todas las columnas de tipo objeto (cadenas de texto)\n",
    "    df.select_dtypes(include='object')\n",
    "\n",
    "    # Seleccionar todas las columnas excepto las num√©ricas\n",
    "    df.select_dtypes(exclude=['int', 'float'])\n",
    "\n",
    "    # Seleccionar todas las columnas excepto las de tipo objeto (cadenas de texto)\n",
    "    df.select_dtypes(exclude='object')\n",
    "    ```\n",
    "\n",
    "- Sacar los nombres de las columnas de tipo categ√≥rico. \n",
    "\n",
    "- Hacer un *for loop* para que nos haga los `.unique()` y los `.value_counts()` de cada columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sacamos las columnas que son de tipo object (categ√≥ricas) de nuestro DataFrame usando el m√©todo '.select_dtypes()'**\n",
    "\n",
    "df_cat = df.select_dtypes(include = \"object\")\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si miramos las columnas seleccionadas por Pandas nos damos cuenta que la mayor√≠a tienen sentido, pero... que pasa con las columnas de 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed' y 'date'. Si excluimos la columna de 'date', vemos claramente que son n√∫meros, ¬øqu√© esta pasando? Y es que, recordad que en Python los decimales se definen con \".\", al ir estos n√∫meros con comas autom√°ticamente nos los convierte en *strings*. Esto ya aprenderemos a cambiarlo en pr√≥ximas lecciones, pero ... ¬øC√≥mo podr√≠amos eliminar esas columnas para esta parte del an√°lisis ya que no son columnas categ√≥ricas? Para eso usaremos el m√©todo `.drop()`. Este m√©todo se utiliza para eliminar filas o columnas de un DataFrame. Tiene la siguiente sintaxis b√°sica:\n",
    "\n",
    "```python\n",
    "DataFrame.drop(labels, axis=0, inplace=False)\n",
    "```\n",
    "\n",
    "- `labels`: Especifica los nombres de las filas o columnas que se queremos eliminar. Puede ser un solo valor o una lista de valores.\n",
    "\n",
    "- `axis`: Indica si se eliminar√°n filas (`axis=0`) o columnas (`axis=1`).\n",
    "\n",
    "- `inplace` (opcional): Es un par√°metro opcional que indica si se realizar√° la eliminaci√≥n directamente en el DataFrame original (`inplace=True`) o si se crear√° un nuevo DataFrame con los cambios (`inplace=False`, valor por defecto).\n",
    "\n",
    "En nuestro caso eliminaremos las columnas de 'ons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed' y 'date'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos las columnas que no son de nuestro inter√©s usando el m√©todo '.drop()'**\n",
    "#pas√°ndolo una lista con las columnas que queremos eliminar\n",
    "#axis = 1 porque los nombres que le hemos pasado en la lista corresponden a las columnas\n",
    "#inplace = True porque queremos que los cambios realizados se sobreescriban en el DataFrame\n",
    "\n",
    "df_cat.drop(['cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed' , 'date'], axis = 1, inplace=True)\n",
    "\n",
    "#chequeamos como se ha quedado el DataFrame y vemos que ya lo tenemos preparado para poder evaluar cuales son sus valores √∫nicos ('.unique()') y sus frecuencias ('.value_counts()')\n",
    "\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creamos una variable con los nombres de las columnas del DataFrame de las variables categ√≥ricas utilizando el m√©todo '.columns'\n",
    "\n",
    "columnas_cat = df_cat.columns\n",
    "print(f\"Las columnas del DataFrame de variables categ√≥ricas son {columnas_cat}\")\n",
    "\n",
    "#empezamos a iterar por cada una de las columnas para sacar sus valores √∫nicos y sus frecuencias\n",
    "\n",
    "for columna in columnas_cat:\n",
    "    print(f\" \\n----------- ESTAMOS ANALIZANDO LA COLUMNA: '{columna.upper()}' -----------\\n\")\n",
    "    print(f\"Sus valores √∫nicos son: {df_cat[columna].unique()}\\n\")\n",
    "    print(f\"Las frecuencias de los valores √∫nicos de las categor√≠as son: {df_cat[columna].value_counts()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOS VALORES NULOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores nulos son aquellos datos que faltan en una columna o variable de nuestro conjunto de datos. \n",
    "Tambi√©n se les conoce como valores faltantes o missing values. Pueden aparecer por muchas razones, como:\n",
    "aparecer por muchas razones, como:\n",
    "\n",
    "- Errores durante la recopilaci√≥n de datos.\n",
    "\n",
    "- Problemas t√©cnicos.\n",
    "\n",
    "- O simplemente porque no hay informaci√≥n disponible en ciertos casos.\n",
    "\n",
    "## ¬øPor qu√© es importante gestionar los valores nulos?\n",
    "**1. Impacto en el an√°lisis:**\n",
    "Los valores nulos pueden afectar tus resultados. Si no los manejas bien, podr√≠as obtener resultados incorrectos o enga√±osos.\n",
    "\n",
    "**2. Precisi√≥n y confiabilidad:**\n",
    "La presencia de valores nulos puede comprometer la precisi√≥n de tus conclusiones. Si no los manejas, tus estimaciones podr√≠an ser inexactas.\n",
    "\n",
    "**3. Integridad de los datos:**\n",
    "Manejar bien los valores nulos asegura que tus datos sean consistentes y completos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos principales de valores nulos en Python\n",
    "Cuando trabajamos con la biblioteca Pandas, hay 5 tipos principales de valores nulos que podemos encontrar:\n",
    "\n",
    "**1.NaN (Not a Number):**\n",
    "Este es el tipo de valor nulo m√°s com√∫n en Pandas y se usa principalmente en columnas num√©ricas.\n",
    "\n",
    "**2. None:** Es el equivalente en Python de decir \"no hay valor\" para cualquier tipo de dato, no solo num√©rico.\n",
    "\n",
    "**3. Valores de texto:**\n",
    "A veces los valores nulos pueden venir en forma de texto. Ejemplos comunes incluyen: \"n/a\", \"NaN\", \"nan\", \"null\": son strings que se introdujeron como valores faltantes.\n",
    "\n",
    "**4. C√≥digos como 99999 o 00000:**\n",
    "En algunos casos, los valores nulos vienen codificados como n√∫meros. Esto es com√∫n en sistemas de medici√≥n autom√°tica (sensores, por ejemplo).\n",
    "\n",
    "**5. NaT (Not a Time):**\n",
    "Este tipo de valor nulo aparece cuando trabajamos con columnas de fechas y horas (datetime)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¬øC√≥mo gestionamos los valores nulos?\n",
    "Gestionar los valores nulos es crucial para que tu an√°lisis sea confiable. En una lecci√≥n futura, aprenderemos a manejar estos valores utilizando Pandas. Algunas de las estrategias m√°s comunes son:\n",
    "\n",
    "- Eliminar las filas o columnas con valores nulos.\n",
    "- Rellenar los valores nulos con datos apropiados (como un valor promedio, la mediana, o un valor espec√≠fico).\n",
    "- Imputaci√≥n avanzada, que implica t√©cnicas estad√≠sticas o modelos m√°s complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©todos para identificar nulos en Pandas\n",
    "1. **isnull():**\n",
    "devuelve un DataFrame o una Serie booleana que indica si cada valor en el DataFrame es nulo o no. Los valores nulos se representan como True, mientras que los valores no nulos se representan como False.Se puede aplicar este m√©todo a un DataFrame completo o a una columna espec√≠fica. Por ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay valores nulos en todo el DataFrame\n",
    "df.isnull()\n",
    "\n",
    "# Verificar si hay valores nulos en una columna espec√≠fica\n",
    "df['columna'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **notnull()**: Similar a isnull(), devuelve un DataFrame o una Serie booleana que indica si cada valor en el DataFrame no es nulo. Los valores no nulos se representan como True y los valores nulos como False. Al igual que isnull(), puedes aplicar este m√©todo a un DataFrame completo o a una columna espec√≠fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay valores no nulos en todo el DataFrame\n",
    "df.notnull()\n",
    "\n",
    "# Verificar si hay valores no nulos en una columna espec√≠fica\n",
    "df['columna'].notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **info()**:  proporciona un resumen conciso de un DataFrame, incluyendo el n√∫mero de valores no nulos en cada columna. Esto puede ser √∫til para identificar r√°pidamente las columnas que tienen valores nulos. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar informaci√≥n resumida del DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **sum()**: Al llamar al m√©todo sum() en un DataFrame o una Serie booleana (como los que nos devuelve el m√©todo .isnull() o .notnull()), se puede obtener el recuento total de valores nulos en cada columna. El valor nulo se trata como True y se suma como 1, mientras que los valores no nulos se tratan como False y se suman como 0. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los valores nulos en cada columna del DataFrame\n",
    "df.isnull().sum()\n",
    "\n",
    "# Contar los valores nulos en una columna espec√≠fica\n",
    "df['columna'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round(df['columna'].isnull().sum() / df.shape * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valores duplicados\n",
    "Los valores duplicados son registros o filas que se repiten en un conjunto de datos, es decir, existen observaciones que tienen exactamente los mismos valores en todas las columnas. Aunque parezcan inofensivos, pueden afectar seriamente la calidad de nuestros an√°lisis. Vamos a ver por qu√© se generan y c√≥mo podemos gestionarlos correctamente. üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øPor qu√© aparecen los valores duplicados? ü§î\n",
    "Existen varias razones por las que pueden aparecer duplicados en nuestros datos:\n",
    "\n",
    "  - Errores de entrada: Durante la recolecci√≥n de datos, pueden ocurrir errores humanos o fallos t√©cnicos que resultan en duplicados involuntarios.\n",
    "\n",
    "  - Fusi√≥n de conjuntos de datos: Al combinar varias fuentes de datos, es posible que algunos registros se repitan si no eliminamos los duplicados correctamente.\n",
    "\n",
    "  - Actualizaciones o modificaciones: Cambios en los registros pueden crear duplicados si no se gestionan bien las actualizaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øPor qu√© es importante gestionar los valores duplicados? üéØ\n",
    " 1.    **Precisi√≥n y validez de los resultados**üéØ\n",
    " Los duplicados pueden distorsionar los resultados de nuestros an√°lisis. Al eliminarlos, aseguramos que trabajamos con datos √∫nicos y obtenemos resultados m√°s confiables. üî¨\n",
    "\n",
    "Por ejemplo, si realizamos una encuesta sobre preferencias alimenticias y permitimos respuestas duplicadas, podr√≠amos obtener una imagen falsa de las preferencias, haciendo que algunos alimentos parezcan m√°s populares de lo que realmente son. Al eliminar duplicados, tenemos una visi√≥n m√°s clara. üçï‚ùåüçï‚û°Ô∏è‚úÖ\n",
    "\n",
    "2. **Eficiencia en el procesamiento ‚ö°**\n",
    "Tener duplicados hace que el conjunto de datos sea m√°s pesado y ralentiza los procesos. Al eliminarlos, optimizamos el rendimiento del an√°lisis y reducimos el consumo de recursos. üöÄ\n",
    "\n",
    "Piensa en analizar los registros de ventas de una tienda online con miles de datos. Si tienes duplicados, el an√°lisis ser√° m√°s lento y requerir√° m√°s potencia de c√≥mputo. Eliminar los duplicados mejorar√° significativamente la velocidad. üíªüí®\n",
    "\n",
    "3. **Consistencia y coherencia de los datos üîÑ**\n",
    "Los duplicados pueden generar inconsistencias que complican la interpretaci√≥n de los datos. Gestionarlos asegura que los datos sean consistentes y m√°s f√°ciles de entender. üìä‚úÖ\n",
    "\n",
    "Imagina que en una base de datos de empleados hay varias entradas duplicadas con diferentes direcciones para la misma persona. Esto puede causar confusi√≥n. Al eliminar los duplicados, garantizamos que la informaci√≥n sea coherente y f√°cil de interpretar. üè¢üë©‚Äçüíºüè†\n",
    "\n",
    "4. **¬øC√≥mo gestionar los valores duplicados? üõ†Ô∏è**\n",
    "Cuando nos encontramos con duplicados, hay varias opciones para gestionarlos. Algunas estrategias que veremos m√°s adelante incluyen:\n",
    "\n",
    "  - Eliminar duplicados: Si los duplicados no aportan valor al an√°lisis, podemos eliminarlos f√°cilmente. üóëÔ∏è\n",
    "\n",
    "  - Agrupar y resumir: En algunos casos, es √∫til agrupar los datos y hacer res√∫menes o agregaciones sobre los duplicados. üìäüîç\n",
    "\n",
    "  - Marcar y analizar: En vez de eliminarlos, podemos a√±adir una columna que marque los duplicados y analizarlos por separado. üìù‚úÖ\n",
    "\n",
    "  - Investigar la causa: Si los duplicados son inesperados, es importante investigar su causa y corregir el origen del problema. Esto puede implicar revisar el proceso de entrada de datos o los sistemas que los generan. üßêüîß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√©todos para identificar los duplicados en Pandas\n",
    "En Pandas, solo podemos identificar los duplicados usando el m√©todo .duplicated() el cual va a identificar las filas duplicadas en un DataFrame y devolver√° una Serie booleana que indica True para las filas duplicadas y False para las filas √∫nicas.\n",
    "\n",
    "Su sintaxis b√°sica es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['columna1', 'columna2'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El m√©todo duplicated() tiene los siguientes par√°metros principales:\n",
    "\n",
    "1.  **subset (opcional)**\n",
    "Este par√°metro permite especificar sobre qu√© columnas queremos comprobar los duplicados. Si deseamos buscar duplicados en columnas espec√≠ficas, podemos pasar una lista con los nombres de dichas columnas.\n",
    "\n",
    "Si no especificamos subset, el m√©todo considerar√° todas las columnas del DataFrame para detectar duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['columna1', 'columna2']) # Busca en 'columna1' y 'columna2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, solo se verificar√° si hay duplicados bas√°ndose en los valores de columna1 y columna2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **keep (opcional)**\n",
    "Este par√°metro controla qu√© ocurrencias de duplicados se marcar√°n como True. Tiene tres opciones principales:\n",
    "\n",
    "   -  first' (valor predeterminado): Marca como True todas las filas duplicadas excepto la primera aparici√≥n de cada conjunto de duplicados. Mantiene la primera fila y marca las siguientes como duplicadas.\n",
    "   \n",
    "   Ejemplo: Si hay 3 filas duplicadas, la primera se mantendr√° y las otras dos se marcar√°n como duplicadas.\n",
    "\n",
    "    -  'last': Marca como True todas las filas duplicadas excepto la √∫ltima aparici√≥n de cada conjunto de duplicados. Es decir, se mantiene la √∫ltima fila y las anteriores se marcan como duplicadas.\n",
    "    \n",
    "    Ejemplo: Si hay 3 filas duplicadas, las primeras dos se marcar√°n como duplicadas y la √∫ltima se mantendr√°.\n",
    "\n",
    "    -   False: Marca todas las filas duplicadas como True, sin excepciones. Esto incluye todas las ocurrencias de duplicados.\n",
    "    Ejemplo: Si hay 3 filas duplicadas, todas se marcar√°n como duplicadas.\n",
    "    \n",
    "    Ejemplo con keep:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(keep='last') # La √∫ltima fila se mantendr√°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Este m√©todo es muy flexible y permite ajustar c√≥mo queremos identificar duplicados en nuestros datos, lo que resulta especialmente √∫til cuando tenemos grandes conjuntos de datos con posibles registros repetidos.\n",
    "\n",
    "¬°No os preocup√©is! Exploraremos estas estrategias con m√°s detalle a medida que avancemos en el bootcamp. Estaremos listos para identificar, gestionar y eliminar valores duplicados con confianza. üí™üìä\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
